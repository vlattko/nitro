{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ResourceExhaustedError` you're encountering indicates that the GPU ran out of memory while trying to allocate a tensor. This can happen when working with large datasets or models that require a lot of memory.\n",
    "\n",
    "There are several ways to address this issue, including reducing the batch size, reducing the size of your model, or using a more powerful GPU with more memory. You can also try using mixed precision training, which can reduce memory usage and speed up training by representing some of the tensors in lower precision.\n",
    "\n",
    "Here's an example of how you can enable mixed precision training in TensorFlow:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "```\n",
    "\n",
    "This code sets the global mixed precision policy to `'mixed_float16'`, which means that some of the tensors will be represented in lower precision to reduce memory usage and speed up training. You can try running this code before creating and training your model to see if it helps resolve the `ResourceExhaustedError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1650, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U slučaju slabe GPU, može se koristiti CPU. Nako n izvršavanja, potrebno je ponovno učitati tensorflow\n",
    "\n",
    "# povratak na GPU:\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Za vraćanje na float32\n",
    "policy = mixed_precision.Policy('float32')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(directory, size=(300, 300), ratio=False, metadata=False, extensions=('.jpg', '.png', '.JPG', '.PNG')):\n",
    "    new_directory = os.path.join(directory, 'resized')\n",
    "    if not os.path.exists(new_directory):\n",
    "        os.makedirs(new_directory)\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(extensions):\n",
    "            image = Image.open(os.path.join(directory, filename))\n",
    "            if not metadata:\n",
    "                # Remove metadata\n",
    "                data = list(image.getdata())\n",
    "                image = Image.new(image.mode, image.size)\n",
    "                image.putdata(data)\n",
    "            if ratio:\n",
    "                width, height = image.size\n",
    "                new_height = int(height * size[0] / width)\n",
    "                image = image.resize((size[0], new_height))\n",
    "            else:\n",
    "                image = image.resize(size)\n",
    "            image.save(os.path.join(new_directory, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=os.path.join(os.getcwd(), 'Images')\n",
    "s = (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_images(directory=dir, size=s, extensions=(\".JPG\"), ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(12340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-precision training\n",
    "def load_images(directory, batch_size=100):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i, filename in enumerate(os.listdir(directory)):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png') or filename.endswith('.JPG') or filename.endswith('.PNG'):\n",
    "            image = tf.io.read_file(os.path.join(directory, filename))\n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = tf.image.resize(image, s) # tu se isto mora prilagoditi input\n",
    "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "            images.append(image)\n",
    "            \n",
    "            label = filename.split(' ')[0]\n",
    "            labels.append(label)\n",
    "        \n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(os.listdir(directory)):\n",
    "            images = tf.stack(images)\n",
    "            labels = tf.stack(labels)\n",
    "            \n",
    "            # Augmentacija udvostručuje broj slika nasumičnim rotacijama i horizontalnim preokretom\n",
    "            data_augmentation = tf.keras.Sequential([\n",
    "                tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "                tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "            ])\n",
    "            \n",
    "            augmented_images = data_augmentation(images)\n",
    "            images = tf.concat([images, augmented_images], axis=0)\n",
    "            labels = tf.concat([labels, labels], axis=0)\n",
    "            \n",
    "            yield images, labels\n",
    "            images = []\n",
    "            labels = []\n",
    "\n",
    "#images, labels = load_images(os.path.join(dir, 'resized'), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plus k-means segmentation\n",
    "def load_images(directory, batch_size=100):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i, filename in enumerate(os.listdir(directory)):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png') or filename.endswith('.JPG') or filename.endswith('.PNG'):\n",
    "            image = tf.io.read_file(os.path.join(directory, filename))\n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = tf.image.resize(image, s) # tu se isto mora prilagoditi input\n",
    "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "            \n",
    "            # Perform image segmentation\n",
    "            segmented_image = segment_image(image)\n",
    "            \n",
    "            # Append the original and segmented images to the list\n",
    "            images.append(image)\n",
    "            images.append(segmented_image)\n",
    "            \n",
    "            label = filename.split(' ')[0]\n",
    "            labels.append(label)\n",
    "            labels.append(label) # Append the label twice since we have two images\n",
    "        \n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(os.listdir(directory)):\n",
    "            images = tf.stack(images)\n",
    "            labels = tf.stack(labels)\n",
    "            \n",
    "            # Augmentacija udvostručuje broj slika nasumičnim rotacijama i horizontalnim preokretom\n",
    "            data_augmentation = tf.keras.Sequential([\n",
    "                tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "                tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "            ])\n",
    "            \n",
    "            augmented_images = data_augmentation(images)\n",
    "            images = tf.concat([images, augmented_images], axis=0)\n",
    "            labels = tf.concat([labels, labels], axis=0)\n",
    "            \n",
    "            yield images, labels\n",
    "            images = []\n",
    "            labels = []\n",
    "            \n",
    "# experiment here with n_clusters for different results\n",
    "def segment_image(image, n_clusters=3):\n",
    "    # Reshape the image into a 2D array of pixels\n",
    "    pixels = tf.reshape(image, (-1, 3))\n",
    "    pixels = pixels.numpy()\n",
    "    \n",
    "    # Fit the KMeans model to the pixel data\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(pixels)\n",
    "    \n",
    "    # Replace each pixel value with its nearest cluster center\n",
    "    segmented_pixels = kmeans.cluster_centers_[kmeans.labels_]\n",
    "    segmented_pixels = segmented_pixels.reshape(image.shape)\n",
    "    \n",
    "    # Convert the segmented image back to a TensorFlow tensor\n",
    "    segmented_image = tf.convert_to_tensor(segmented_pixels)\n",
    "    \n",
    "    return segmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available physical devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "TensorFlow is running on the CPU\n"
     ]
    }
   ],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available physical devices:\", physical_devices)\n",
    "\n",
    "# Check if a CPU device is available\n",
    "cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "if len(cpu_devices) > 0:\n",
    "    print(\"TensorFlow is running on the CPU\")\n",
    "else:\n",
    "    print(\"TensorFlow is not running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = os.path.join(dir, 'resized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    unique_labels, _ = tf.unique(labels)\n",
    "    label_to_index = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(unique_labels, tf.range(tf.size(unique_labels))),\n",
    "        -1\n",
    "    )\n",
    "    encoded_labels = label_to_index.lookup(labels)\n",
    "    return encoded_labels.numpy() # Convert to NumPy array\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for images, labels in load_images(image_directory, batch_size=16):\n",
    "    if images.shape[0] > 0: # Check if the batch is not empty\n",
    "        all_images.append(images)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_images = tf.concat(all_images, axis=0)\n",
    "all_labels = tf.concat(all_labels, axis=0)\n",
    "\n",
    "# Encode the labels\n",
    "all_labels = encode_labels(all_labels)\n",
    "\n",
    "# Reshape the images to 2D\n",
    "all_images = tf.reshape(all_images, (all_images.shape[0], -1))\n",
    "\n",
    "# Balance the classes using oversampling\n",
    "ros = RandomOverSampler()\n",
    "balanced_images, balanced_labels = ros.fit_resample(all_images.numpy(), all_labels)\n",
    "\n",
    "# Convert the labels to integer type\n",
    "balanced_labels = balanced_labels.astype(int)\n",
    "\n",
    "# Convert the data to TensorFlow tensors\n",
    "balanced_images = tf.convert_to_tensor(balanced_images)\n",
    "balanced_labels = tf.convert_to_tensor(balanced_labels)\n",
    "\n",
    "# Reshape the images back to 4D\n",
    "balanced_images = tf.reshape(balanced_images, (-1, s[0], s[1], 3))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "indices = tf.range(start=0, limit=balanced_images.shape[0], dtype=tf.int32)\n",
    "train_indices, test_indices = tf.split(indices, num_or_size_splits=[int(0.8*balanced_images.shape[0]), int(0.2*balanced_images.shape[0])])\n",
    "train_images = tf.gather(balanced_images, train_indices)\n",
    "test_images = tf.gather(balanced_images, test_indices)\n",
    "train_labels = tf.gather(balanced_labels, train_indices)\n",
    "test_labels = tf.gather(balanced_labels, test_indices)\n",
    "\n",
    "# Convert the data back to TensorFlow tensors\n",
    "train_images = tf.convert_to_tensor(train_images)\n",
    "test_images = tf.convert_to_tensor(test_images)\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "test_labels = tf.convert_to_tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "\n",
    "class UNet(Model):\n",
    "    def __init__(self, input_shape=(300, 300, 3), num_classes=10):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(256, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2))\n",
    "        ])\n",
    "\n",
    "        # Bridge\n",
    "        self.bridge = Conv2D(512, (3, 3), activation='relu')\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            UpSampling2D((2, 2)),\n",
    "            Conv2D(256, (3, 3), activation='relu'),\n",
    "            Concatenate([self.decoder.output, self.encoder.layers[-3].output]),\n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            UpSampling2D((2, 2)),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            Concatenate([self.decoder.output, self.encoder.layers[-4].output]),\n",
    "            Conv2D(32, (3, 3), activation='relu'),\n",
    "            UpSampling2D((2, 2)),\n",
    "            Conv2D(num_classes, (3, 3), activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_output = self.encoder(inputs)\n",
    "        bridge_output = self.bridge(encoder_output)\n",
    "        decoder_output = self.decoder(bridge_output)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UNet' object has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Korisnik\\OneDrive - Poljoprivredni institut Osijek\\UZI mjerenja\\2023\\network.ipynb Cell 18\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m UNet()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mfit(train_images, train_labels, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Korisnik\\OneDrive - Poljoprivredni institut Osijek\\UZI mjerenja\\2023\\network.ipynb Cell 18\u001b[0m line \u001b[0;36mUNet.__init__\u001b[1;34m(self, input_shape, num_classes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbridge \u001b[39m=\u001b[39m Conv2D(\u001b[39m512\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Decoder\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     UpSampling2D((\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     Conv2D(\u001b[39m256\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     Concatenate([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39moutput, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39moutput]),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     Conv2D(\u001b[39m128\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     UpSampling2D((\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     Conv2D(\u001b[39m64\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     Concatenate([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39moutput, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39moutput]),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     Conv2D(\u001b[39m32\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     UpSampling2D((\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     Conv2D(num_classes, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X53sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m ])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'UNet' object has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "# Load the new image\n",
    "new_image = tf.io.read_file('new_image.jpg')\n",
    "new_image = tf.image.decode_jpeg(new_image, channels=3)\n",
    "new_image = tf.image.resize(new_image, s)\n",
    "new_image = tf.image.convert_image_dtype(new_image, tf.float32)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(new_image)[0]\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = tf.argmax(prediction)\n",
    "\n",
    "# Print the predicted class\n",
    "print('Predicted class:', predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 36, 72, 256), (None, 43, 43, 256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Korisnik\\OneDrive - Poljoprivredni institut Osijek\\UZI mjerenja\\2023\\network.ipynb Cell 21\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Create the U-Net model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m input_shape \u001b[39m=\u001b[39m (\u001b[39m300\u001b[39m, \u001b[39m300\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m model \u001b[39m=\u001b[39m unet(input_shape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\Korisnik\\OneDrive - Poljoprivredni institut Osijek\\UZI mjerenja\\2023\\network.ipynb Cell 21\u001b[0m line \u001b[0;36munet\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m up7 \u001b[39m=\u001b[39m Conv2DTranspose(\u001b[39m256\u001b[39m, (\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), strides\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(conv6)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m crop7 \u001b[39m=\u001b[39m Cropping2D(cropping\u001b[39m=\u001b[39m((\u001b[39m16\u001b[39m, \u001b[39m16\u001b[39m), (\u001b[39m16\u001b[39m, \u001b[39m16\u001b[39m)))(conv3)  \u001b[39m# Adjust cropping dimensions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m up7 \u001b[39m=\u001b[39m concatenate([up7, crop7], axis\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m conv7 \u001b[39m=\u001b[39m Conv2D(\u001b[39m256\u001b[39m, \u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(up7)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X44sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m conv7 \u001b[39m=\u001b[39m Conv2D(\u001b[39m256\u001b[39m, \u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(conv7)\n",
      "File \u001b[1;32mc:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\merging\\concatenate.py:217\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mkeras.layers.concatenate\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcatenate\u001b[39m(inputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    187\u001b[0m   \u001b[39m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[39m  >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39m      A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   \u001b[39mreturn\u001b[39;00m Concatenate(axis\u001b[39m=\u001b[39;49maxis, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)(inputs)\n",
      "File \u001b[1;32mc:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\merging\\concatenate.py:123\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    120\u001b[0m unique_dims \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[0;32m    121\u001b[0m     shape[axis] \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shape_set \u001b[39mif\u001b[39;00m shape[axis] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unique_dims) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 123\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 36, 72, 256), (None, 43, 43, 256)]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, Conv2DTranspose, Cropping2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the U-Net architecture\n",
    "def unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    \n",
    "    # Bottom\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    # Decoder\n",
    "    conv4_padded = ZeroPadding2D(padding=((1, 1), (1, 1)))(conv4)\n",
    "    crop6 = Cropping2D(cropping=((0, 0), (0, 0)))(conv4_padded)\n",
    "\n",
    "    up6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(drop5)\n",
    "    crop6 = Cropping2D(cropping=((2, 1), (2, 1)))(conv4_padded)\n",
    "    up6 = concatenate([up6, crop6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = Conv2DTranspose(256, (1,1), strides=(1, 2), padding='same')(conv6)\n",
    "    crop7 = Cropping2D(cropping=((16, 16), (16, 16)))(conv3)  # Adjust cropping dimensions\n",
    "    up7 = concatenate([up7, crop7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "    crop8 = Cropping2D(cropping=((40, 40), (40, 40)))(conv2)  # Adjust cropping dimensions\n",
    "    up8 = concatenate([up8, crop8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
    "    crop9 = Cropping2D(cropping=((88, 88), (88, 88)))(conv1)  # Adjust cropping dimensions\n",
    "    up9 = concatenate([up9, crop9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the U-Net model\n",
    "input_shape = (300, 300, 3)\n",
    "model = unet(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using your training data\n",
    "model.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'data_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Korisnik\\OneDrive - Poljoprivredni institut Osijek\\UZI mjerenja\\2023\\network.ipynb Cell 19\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Define the U-Net model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m input_shape \u001b[39m=\u001b[39m (s[\u001b[39m0\u001b[39m], s[\u001b[39m1\u001b[39m], \u001b[39m3\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m unet(input_shape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\Korisnik\\OneDrive - Poljoprivredni institut Osijek\\UZI mjerenja\\2023\\network.ipynb Cell 19\u001b[0m line \u001b[0;36munet\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m data_format_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(data_format)\u001b[39m.\u001b[39mlower()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Remove the data_format argument from the call() method\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m crop6 \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mCropping2D((\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), (\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), data_format\u001b[39m=\u001b[39;49mdata_format_str)(up6)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m concat6 \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mconcatenate([crop6, conv4], axis\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m conv6 \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m512\u001b[39m, \u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(concat6)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'data_format'"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "dataset = load_images(image_directory, batch_size)\n",
    "\n",
    "# Define the U-Net model\n",
    "input_shape = (s[0], s[1], 3)\n",
    "model = unet(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "for images, labels in dataset:\n",
    "    model.fit(train_images, train_labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 2400\n",
      "Average per channel: 113.18425\n",
      "Standard deviation per channel: 66.900085\n",
      "Min per channel: 0.0\n",
      "Max per channel: 255.0\n"
     ]
    }
   ],
   "source": [
    "# info about all loaded images\n",
    "print('Number of images:', all_images.shape[0])\n",
    "# average per channel rgb\n",
    "print('Average per channel:', np.mean(all_images, axis=(0, 1)))\n",
    "# standard deviation per channel\n",
    "print('Standard deviation per channel:', np.std(all_images, axis=(0, 1)))\n",
    "# min per channel\n",
    "print('Min per channel:', np.min(all_images, axis=(0, 1)))\n",
    "# max per channel\n",
    "print('Max per channel:', np.max(all_images, axis=(0, 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''all_images = []\n",
    "all_labels = []\n",
    "for images, labels in load_images(os.path.join(os.getcwd(), 'resized'), batch_size=16):\n",
    "    all_images.append(images)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_images = tf.concat(all_images, axis=0)\n",
    "all_labels = tf.concat(all_labels, axis=0)\n",
    "\n",
    "# Encode the labels\n",
    "all_labels = encode_labels(all_labels)\n",
    "\n",
    "# Compute the class distribution\n",
    "_, class_counts = tf.unique_with_counts(all_labels)\n",
    "max_count = tf.reduce_max(class_counts)\n",
    "\n",
    "# Oversample the minority classes\n",
    "balanced_images = []\n",
    "balanced_labels = []\n",
    "for i in range(tf.size(class_counts)):\n",
    "    class_mask = tf.equal(all_labels, i)\n",
    "    class_images = tf.boolean_mask(all_images, class_mask)\n",
    "    class_labels = tf.boolean_mask(all_labels, class_mask)\n",
    "    \n",
    "    num_samples = tf.shape(class_images)[0]\n",
    "    num_extra_samples = max_count - num_samples\n",
    "    \n",
    "    extra_indices = tf.random.uniform((num_extra_samples,), minval=0, maxval=num_samples, dtype=tf.int32)\n",
    "    extra_images = tf.gather(class_images, extra_indices)\n",
    "    extra_labels = tf.gather(class_labels, extra_indices)\n",
    "    \n",
    "    balanced_images.append(tf.concat([class_images, extra_images], axis=0))\n",
    "    balanced_labels.append(tf.concat([class_labels, extra_labels], axis=0))\n",
    "\n",
    "balanced_images = tf.concat(balanced_images, axis=0)\n",
    "balanced_labels = tf.concat(balanced_labels, axis=0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "num_samples = balanced_images.shape[0]\n",
    "num_train_samples = int(num_samples * 0.8)\n",
    "\n",
    "train_images = balanced_images[:num_train_samples]\n",
    "train_labels = balanced_labels[:num_train_samples]\n",
    "\n",
    "test_images = balanced_images[num_train_samples:]\n",
    "test_labels = balanced_labels[num_train_samples:]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 13:51:34.928637: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of 3 which is outside the valid range of [0, 3).  Label values: 3 3 0 2 2 3 3 2 0 3 0 2 2 2 1 0 1 2 3 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/81/dk1hscqs2slg5m53bqfbpj8c0000gn/T/ipykernel_6343/805229020.py\", line 29, in <module>\n      history = model.fit(train_images, train_labels, batch_size=20, epochs=100)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of 3 which is outside the valid range of [0, 3).  Label values: 3 3 0 2 2 3 3 2 0 3 0 2 2 2 1 0 1 2 3 0\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_72187]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/81/dk1hscqs2slg5m53bqfbpj8c0000gn/T/ipykernel_6343/805229020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m '''\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/81/dk1hscqs2slg5m53bqfbpj8c0000gn/T/ipykernel_6343/805229020.py\", line 29, in <module>\n      history = model.fit(train_images, train_labels, batch_size=20, epochs=100)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 918, in compute_loss\n      return self.compiled_loss(\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/losses.py\", line 1862, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/Users/vlatko/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/backend.py\", line 5202, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of 3 which is outside the valid range of [0, 3).  Label values: 3 3 0 2 2 3 3 2 0 3 0 2 2 2 1 0 1 2 3 0\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_72187]"
     ]
    }
   ],
   "source": [
    "# složeniji model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(s[0], s[1], 3)), # Tu promijeni size\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        #tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    '''model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])'''\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "\n",
    "history = model.fit(train_images, train_labels, batch_size=20, epochs=100)\n",
    "'''\n",
    "for images, labels in load_images(os.path.join(os.getcwd(), 'resized'), batch_size=16):\n",
    "    labels = encode_labels(labels)\n",
    "    model.fit(images, labels, batch_size = 32, epochs=5)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO model with  24 convolutional layers followed by 2 fully connected layers (FC). Some convolution layers use 1 × 1 reduction layers alternatively to reduce the depth of the features maps. For the last convolution layer, it outputs a tensor with shape (7, 7, 1024)\n",
    "def create_yolo():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(s[0], s[1], 3)), # Tu promijeni size\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(1024, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dense(1470, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv2d_12\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_12/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_12/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,256], [3,3,256,512].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/81/dk1hscqs2slg5m53bqfbpj8c0000gn/T/ipykernel_6343/3751011266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0myolo_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/81/dk1hscqs2slg5m53bqfbpj8c0000gn/T/ipykernel_6343/1570818399.py\u001b[0m in \u001b[0;36mcreate_yolo\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOLO model with  24 convolutional layers followed by 2 fully connected layers (FC). Some convolution layers use 1 × 1 reduction layers alternatively to reduce the depth of the features maps. For the last convolution layer, it outputs a tensor with shape (7, 7, 1024)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     model = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Tu promijeni size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   2011\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_12\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_12/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_12/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,256], [3,3,256,512].\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "yolo = create_yolo()\n",
    "yolo_history = yolo.fit(train_images, train_labels, batch_size=20, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Korisnik\\OneDrive - Poljoprivredni institut Osijek\\UZI mjerenja\\2023\\network.ipynb Cell 21\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mmodel.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# save history = model.fit(train_images, train_labels, batch_size=20, epochs=100)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Korisnik/OneDrive%20-%20Poljoprivredni%20institut%20Osijek/UZI%20mjerenja/2023/network.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39m\u001b[39mhistory.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# plot the model tensorflow\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "#save the model and weights\n",
    "model.save('model.h5')\n",
    "# save history = model.fit(train_images, train_labels, batch_size=20, epochs=100)\n",
    "history.save('history.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_1455/conv2d_132/Conv2D' defined at (most recent call last):\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 461, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 450, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in dispatch_shell\n      await result\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 652, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2768, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2814, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3012, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3251, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Korisnik\\AppData\\Local\\Temp\\ipykernel_23624\\1604357493.py\", line 57, in <module>\n      model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint_callback])\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 250, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 225, in convolution_op\n      return tf.nn.convolution(\nNode: 'sequential_1455/conv2d_132/Conv2D'\nOOM when allocating tensor with shape[16,128,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1455/conv2d_132/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1563836]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\Umjeravanje\\Slike\\resize.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Umjeravanje/Slike/resize.ipynb#X33sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m ModelCheckpoint(checkpoint_filename, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Umjeravanje/Slike/resize.ipynb#X33sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Train the model on the training data for this fold\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/Umjeravanje/Slike/resize.ipynb#X33sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback])\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Umjeravanje/Slike/resize.ipynb#X33sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Load the best model from the checkpoint file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Umjeravanje/Slike/resize.ipynb#X33sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(checkpoint_filename)\n",
      "File \u001b[1;32mc:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1455/conv2d_132/Conv2D' defined at (most recent call last):\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 461, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 450, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in dispatch_shell\n      await result\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 652, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2768, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2814, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3012, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3251, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Korisnik\\AppData\\Local\\Temp\\ipykernel_23624\\1604357493.py\", line 57, in <module>\n      model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint_callback])\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 250, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 225, in convolution_op\n      return tf.nn.convolution(\nNode: 'sequential_1455/conv2d_132/Conv2D'\nOOM when allocating tensor with shape[16,128,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1455/conv2d_132/Conv2D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1563836]"
     ]
    }
   ],
   "source": [
    "# CV model\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(100, 100, 3)), # Tu promijeni size\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        #tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "# Define the number of folds\n",
    "k = 5\n",
    "\n",
    "# Split the data into K folds\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize an empty list to store the accuracy scores for each fold\n",
    "scores = []\n",
    "\n",
    "\n",
    "    \n",
    "# Initialize an empty list to store the models for each fold\n",
    "models = []\n",
    "\n",
    "# Loop over the folds\n",
    "for train_index, test_index in kf.split(train_images):\n",
    "    # Convert the indices to integer type\n",
    "    train_index = train_index.astype(int)\n",
    "    test_index = test_index.astype(int)\n",
    "    \n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = tf.gather(train_images, train_index), tf.gather(train_images, test_index)\n",
    "    y_train, y_test = tf.gather(train_labels, train_index), tf.gather(train_labels, test_index)    \n",
    "    # Create the model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Define the checkpoint filename for this fold\n",
    "    checkpoint_filename = 'model_fold_' + str(len(models)) + '.h5'\n",
    "    \n",
    "    # Define the checkpoint callback to save the best model during training\n",
    "    checkpoint_callback = ModelCheckpoint(checkpoint_filename, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "    \n",
    "    # Train the model on the training data for this fold\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint_callback])\n",
    "    \n",
    "    # Load the best model from the checkpoint file\n",
    "    model.load_weights(checkpoint_filename)\n",
    "    \n",
    "    # Evaluate the model on the testing data for this fold\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Append the accuracy score to the list of scores\n",
    "    scores.append(score[1])\n",
    "    \n",
    "    # Append the model to the list of models\n",
    "    models.append(model)\n",
    "    \n",
    "# Calculate the mean and standard deviation of the accuracy scores\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "print('Mean accuracy:', mean_score)\n",
    "print('Standard deviation:', std_score)\n",
    "\n",
    "# Select the best model based on the mean accuracy score\n",
    "best_model_index = np.argmax(scores)\n",
    "best_model = models[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4114583432674408]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovaj radi i grid search\n",
    "\n",
    "# Define the number of folds\n",
    "k = 5\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100, 200],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Split the data into K folds\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize an empty list to store the accuracy scores for each fold\n",
    "scores = []\n",
    "\n",
    "# Loop over the hyperparameters\n",
    "for batch_size in param_grid['batch_size']:\n",
    "    for epochs in param_grid['epochs']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            # Loop over the folds\n",
    "            for train_index, test_index in kf.split(train_images):\n",
    "                # Split the data into training and testing sets for this fold\n",
    "                X_train, X_test = train_images[train_index], train_images[test_index]\n",
    "                y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "                # Create the model\n",
    "                model = create_model()\n",
    "\n",
    "                # Set the hyperparameters\n",
    "                model.optimizer.learning_rate = learning_rate\n",
    "\n",
    "                # Train the model on the training data for this fold\n",
    "                model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "                # Evaluate the model on the testing data for this fold\n",
    "                score = model.evaluate(X_test, y_test)\n",
    "\n",
    "                # Append the accuracy score to the list of scores\n",
    "                scores.append(score[1])\n",
    "\n",
    "            # Calculate the mean and standard deviation of the accuracy scores for this set of hyperparameters\n",
    "            mean_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "\n",
    "            # Print the hyperparameters and the corresponding mean accuracy and standard deviation\n",
    "            print('batch_size:', batch_size, 'epochs:', epochs, 'learning_rate:', learning_rate, 'mean accuracy:', mean_score, 'standard deviation:', std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 49ms/step\n",
      "Test accuracy: 0.18\n",
      "Test precision: 0.33\n",
      "Test recall: 0.06\n",
      "Test F1-score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Korisnik\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzqElEQVR4nO3deXhU5dn48e+dyb6TjSUJhCWsskcUsYpQLYiVumOtQl9bl2q1rdZX+mutr/Xt21ar1VbbonXBfa/UUhGURQWEsG8hYQkkIWQBspE98/z+mJM4hCwTyGQmM/fnuuZizjJn7sOBc8+znOcRYwxKKaX8V4CnA1BKKeVZmgiUUsrPaSJQSik/p4lAKaX8nCYCpZTyc5oIlFLKz2kiUH5BRNJExIhIoAv7LhCRL3oiLqW8gSYC5XVEJFdE6kUkodX6LdbNPM1DoSnlkzQRKG91ELixeUFExgLhngvHO7hSolGqqzQRKG/1CnCL0/J8YLHzDiISIyKLRaRERA6JyC9FJMDaZhORx0WkVEQOAHPa+Ow/RKRQRApE5FERsbkSmIi8IyJHRaRcRNaIyBinbWEi8kcrnnIR+UJEwqxtF4rIWhEpE5E8EVlgrV8lIj9wOsYpVVNWKeguEckBcqx1T1nHqBCRTSLyDaf9bSLyCxHZLyKV1vZUEXlGRP7Y6lyWiMhPXTlv5bs0EShvtR6IFpFR1g16HvBqq33+DMQAQ4CLcSSO71vbfghcAUwEMoBrW332JaARGGbtcxnwA1zzHyAdSAI2A685bXscmAxcAMQBDwB2ERlkfe7PQCIwAdjq4vcBfAc4DxhtLW+0jhEHvA68IyKh1raf4ShNXQ5EA/8FVAMvAzc6JcsE4JvW55U/M8boS19e9QJycdygfgn8HzALWA4EAgZIA2xAPTDa6XO3A6us958Bdzhtu8z6bCDQF6gDwpy23wistN4vAL5wMdZY67gxOH5Y1QDj29hvIfBBO8dYBfzAafmU77eOP6OTOE40fy+wF5jbzn57gEut93cDSz19vfXl+ZfWNypv9gqwBhhMq2ohIAEIAg45rTsEJFvvBwB5rbY1G2R9tlBEmtcFtNq/TVbp5H+B63D8src7xRMChAL72/hoajvrXXVKbCJyP3ArjvM0OH75Nzeud/RdLwPfw5FYvwc8dRYxKR+hVUPKaxljDuFoNL4ceL/V5lKgAcdNvdlAoMB6X4jjhui8rVkejhJBgjEm1npFG2PG0LnvAnNxlFhicJROAMSKqRYY2sbn8tpZD3CSUxvC+7WxT8swwVZ7wAPA9UAfY0wsUG7F0Nl3vQrMFZHxwCjgn+3sp/yIJgLl7W7FUS1y0nmlMaYJeBv4XxGJsurgf8bX7QhvA/eISIqI9AEedPpsIfAJ8EcRiRaRABEZKiIXuxBPFI4kcgzHzfu3Tse1Ay8AT4jIAKvRdqqIhOBoR/imiFwvIoEiEi8iE6yPbgWuFpFwERlmnXNnMTQCJUCgiDyEo0TQ7HngNyKSLg7jRCTeijEfR/vCK8B7xpgaF85Z+ThNBMqrGWP2G2My29n8Yxy/pg8AX+Bo9HzB2vYcsAzYhqNBt3WJ4hYgGNiNo379XaC/CyEtxlHNVGB9dn2r7fcDO3DcbI8DvwcCjDGHcZRs7rPWbwXGW595Ekd7RxGOqpvX6Ngy4GMg24qlllOrjp7AkQg/ASqAfwBhTttfBsbiSAZKIcboxDRK+RMRuQhHyWmQ0RuAQksESvkVEQkC7gWe1ySgmmkiUMpPiMgooAxHFdifPBqM8ipaNaSUUn7ObSUCEXlBRIpFZGc720VEnhaRfSKyXUQmuSsWpZRS7XPnA2UvAX/h9AeBms3G8Zh+Oo5H5/9q/dmhhIQEk5aW1j0RKqWUn9i0aVOpMSaxrW1uSwTGmDWdDBc8F1hsNVitF5FYEelv9fFuV1paGpmZ7fUmVEop1RYROdTeNk82Fidzat/nfL4eHuAUInKbiGSKSGZJSUmPBKeUUv6iV/QaMsYsMsZkGGMyEhPbLNkopZQ6Q55MBAWcOhZMCl+PE6OUUqqHeHL00SXA3SLyJo5G4vLO2gfa09DQQH5+PrW1td0aoDcKDQ0lJSWFoKAgT4eilPIRbksEIvIGMB1IEJF84Nc4hv7FGPM3YCmOsVf24Zg04/ttH6lz+fn5REVFkZaWhtOwwj7HGMOxY8fIz89n8ODBng5HKeUj3Nlr6MZOthvgru74rtraWp9PAgAiQnx8PNpgrpTqTr2isdgVvp4EmvnLeSqleo7OUKaU8hsllXX8c0sBseFBDEmMYGhiJLHhwW75LrvdcKK6nqDAAKJDXWvTO1pey/6SKjLS+hASaHNLXG3RRNANjh07xsyZMwE4evQoNpuN5m6uGzZsIDi4/X9omZmZLF68mKeffrpHYlXKH9U2NPHil7k8s3IfVXWNLesDBP7v6rHccO7ADj7dubrGJjYdOsGGg8fZmHuc7KIqjp+sp8luiIsIZvXPpxPVRjIoqqjly32lfLnvGBtyj5F33DFPUP+YUO6eMYzrJqcSHOj+ihtNBN0gPj6erVu3AvDwww8TGRnJ/fff37K9sbGRwMC2/6ozMjLIyMjoiTCV8lon6xr5YEsBb23MY/qIRO67bES7+5ZV11NaVcewpKh298k7Xs3q7BLyTlRTcKKGLYfLKCir4ZujkvjvWSMJtAVwsLSKRWsO8NCHuxiXEsuo/tHtHq8jh49Vc+vLG8kprkIERvWL5pIRiSRGhRBkC+BPK3J4/avD3H7x17OH7iwo5/53tpF1tBKAuIhgzk3rw/ypafSPCeP5Lw7w/z7YybMr93Pz1EFcMymFxKiQM4rPFZoI3GTBggWEhoayZcsWpk2bxrx587j33nupra0lLCyMF198kREjRrBq1Soef/xxPvroIx5++GEOHz7MgQMHOHz4MD/5yU+45557PH0qyss02Q21DU3UN9qpaWgi73g1B0tPknusmgCBqNAgosMCOX9IPEMTI7t8fGMM5TUNRIUGYQvouE2qscnOS2tzef2rwzx61TlcMDShy9/3whcHeXJ5NpV1jYQF2cg/Uc1Pvjm8ze/enl/GbYs3UVJVx++vGce1k1NatlXXO5LJP7cUsDH3BABBNmFAbBhDkyL5/TXjuDD96/gGJ0QwLiWW2U99zt2vb2bJ3RcSEeK4JVbVNVLb0ERIYAAhgTaCbNJm+9xXB45xx6ubMMBfvjuRi4YnnlYNtDH3OM9/cZD5F6QRGmSjscnOz9/dzrGT9Tw4eyQXDktgdP9oApzO9/Kx/VidXcKzK/fzu/9k8fiyvcwYmcQd04cyaWCfLv8dd8bnEsH//GsXu49UdOsxRw+I5tffdmVe81Pl5+ezdu1abDYbFRUVfP755wQGBrJixQp+8Ytf8N577532maysLFauXEllZSUjRozgzjvv1GcGVIt9xZXc8Pf1HDtZf9q2IJtgDDTaHUPLi8Dsc/rxo+nDOCc5xqXj1zY0cffrm1mxp5gAgbiIEGLCAgmwboJhwTampMVxYXoCkSGB/HrJLnYdqSAyJJDbFm/irdvPZ8yAU7+rvtHOZ1lFfLzzKN+fNpjxqbEt2xqb7PxhWRbnDIhh4eWjKCir4Z43trA1r4zJg0694b2/OZ8H399BYmQIGYP6cP872yiurOWOi4by/pYC/vBxFsWVdQxLiuTn3xrBnLH9GRgXfsoNtrWEyBCemjeB7z3/Fb/6cCffHj+AtzfmsXx3UcvfI0BoUAAJkSEkRIbQJzyIqNAgggMD+HBrAalx4bww/1zSEiLa/I4fTR/GTc9/xfubC/jueQN57avD7Cms4JnvTmLOuLZnRxURpo9IYvqIJPYVV/FOZh7vbc6n4ESNJoLe5rrrrsNmczT4lJeXM3/+fHJychARGhoa2vzMnDlzCAkJISQkhKSkJIqKikhJSWlzX9Vzahua+N1/sjh2sp7k2DCSY0OZPiKJ1Ljwln1KKuv46VtbOVJWw1UTk7k2I4Xw4ECWbC3grcw8iirquPycflw1KYXxKTFd7gFmtxsWvr+DJmN4cPbIll+rA2JDGZoYyYDYMAIEahvslFbV8ebGwyxee4ilO45yfUYK/3vVWIJs7dc3n6xr5IeLM1l34Bi3XTSEYFsApVV1VNR+/W/1+Ml6Fq8/xPNfHAQgKSqEZ2+axMSBsVzz7FoWvLiR9+64gNS4MLbnl/PR9iO8v7mgJXFFhASekgj2l5yktsHOTecPZPKgPgxLjMQWIKzMKj4lETz/+QEe/fcezh8SxzPfnURUaBD3v7ONP3y8l1fXHeJIeS0TUmN55qZJZAzq06W/2wuGJnDPzHT+tCKH9zcXEBcRzPwL0kiLD6eu0U5do92qjnJUSZVW1XOw9CQVtY1MH5HE49eNJyas/R9rFwyNZ1xKDH9fs59vjkrij5/sZdqweC4f28+l+IYlRbLw8lHc/632q8vOls8lgjP55e4uERFf/0L41a9+xSWXXMIHH3xAbm4u06dPb/MzISFf1wPabDYaGxvb3E+5prahiX3FVQzvG9XS6Fbb0MRH2wtZsbuI2PAgkmPDGBgfzuVj+7d5o2xosrf8Sk6NC+PjnYU0NBlCAvdwz8x0brtoCDlFVfzg5Y2cqG5gbHIMf1yezZMrsgm0BVDfaGd0/2gmD+zDGxvzeHndISYNjOXt26cS6PR99Y12Nhw8zrmD2+4x8lZmHhtzT/CHa8dxfUbqadubhQXbSI0L5+ffGsntFw/l2ZX7+dvq/RytqOOvN01qqf5wVlHbwPdf3MiWwyd44vrxXDWx/R8fNfVNbMg9zuFjJ5k7MbmlKmTxrVO49m/rmLdoHSJCQVkNgQHCzFFJzDt3IH9fs59t+WWnHGtnQTkAY60SS0x4EJMH9eGzrOKWG19tQxN/WbmPi4Yn8o/5GS3X6E83TKB/bCjLdxXxpxsmcOX4AR3++u/Ij2ekE2QLYHBCBN8c1bdbG2hFhB9NH8odr25m3nPrqa5v4n+uHNPlHwIdJfGz5XOJwFuVl5eTnOwYXPWll17ybDC9SG7pSR799x5S48J46IrRp/zn2VlQTv6JGmadc/ovq5LKOl5Zl8sr6w9xorqB8GAb5w2OIzUunH9tO8KJ6gYGxIRS32SntMrxa9VuzGk3wCa74advbWXFnmJ+M3cMN09Nw243HD5eze/+k8Vjy/bywZYCjpTVEB0axDt3TOWc5BgOHTvJu5vyqapr5JpJKS1VMxW1Dbyy7hCPLdvLsl1Fp1QN/HXVfp5ckU1SVAjzL0jjpvMGtnRtLK6s5bdLHb+Ir5vsegkxOjSIB2ePZHBCOL/4YCfzFq3nhQXnntLwmHW0grte28zh49U8891JzB7bdnVFs7BgGxcPTwROHQByWFIU/5h/Lne/vplR/aL46aXDuXRUX2LCHYliQ+5xnltzgNqGJkKDHIluR0E5YUE2Bid83ZYxY2QSv/tPFoXlNfSPCWPJtiOUVTdw58VDT7kZBgQIC2ePYuHsUS7/fbTHFiDcdcmwsz5Oey4b3Y8hiREcKDnJ7RcN6bCh2xM0EfSQBx54gPnz5/Poo48yZ84cT4fj9eob7Sxas5+nP9uH3W5otBtG9Y9u+SV8sPQk331uPRW1jTwydwy3TE0DHA2df1t9gCdXZNPQZGfmyL7MOqcf2/PL+CKnlDU5pXxzVBLzp6YxdWg8IkJNfRNTf/cpX+47dloi+MX7O/hoeyELZ4/kZus7AgKEtIQI/nbzZJbvLuLhJbtI7xvFopsn0zc6FIBB8RFt9nyJDg3ijouH8tbGPF748mBLIqiub+SltQeZNDCWiJBAHlu2l6c/zWHK4Di+kZ7AhoMnqGu089urxp7RQ4U3nDuQxKgQfvTaZqY/tpKrJiVzy9Q0Nh86wa+X7CI6LIjF/3UeU4fGd/nYziYP6sO6hTPb3DY+JYZGu2FPYQUTrXruXUfKGT0g+pSG4eZEsDKrhBunpPLKukMM7xvJ+UPizio2TwoIEH51xWheXpvLj2emezqc02gi6GYPP/xwm+unTp1KdnZ2y/Kjjz4KwPTp01uqiVp/dufONmf59Hm5pSe587XN7Cms4PKx/fjVFaO57+1tPPThTiakxtI/JpQfLs7EFiBcNDyRhz7cRWiQjbkTBrDwvR28v6WAWWP68cCsEQyxes009y5paLKfVsQOC7Zx/uB41u0/hjGm5Ua7v6SKtzLz+MGFg0/p+ufs0tF9mTEyCQGXqyVsAcKCC9J45KPdbM8vY1xKLG9syONEdQPPzxnF5EFxZB2t4M0NeXyxr5TfLs0C4GeXDm85nzMxY2RfPrzrQv6+Zj9vZ+bz6vrDAFw4LIEnb5jg1u6JQEvbwLa8MiYO7EOT3bDrSMVp1VzpSZEkx4bxWVYxo/pHsaOgnN/M7XpVire5ZEQSl4xI8nQYbdJEoLpdUUUtS7YeYdOhE9w8dRDThrnepXD57iJ+9vZWbAHCopsnc9kYR7XPn26YwOVPf85dr20mNS6cg6UneeXWKUwe1IcfvJzJg+9t5/nPD5BdVMXPLh3Oj2cMa/PG0V4969Sh8Xy86yh5x2sYGO9oAP50TxEAC6aldRhzZ10s23JdRgpPLM/mxS9z+f0143j+8wNMGRzH5EGOX70j+0Xz8JWO9q7C8hqyCiv5RnrXu2a2NqJfFE9cP4FfzhnNe5vyCQu2ceOUgWd0Dl3VLzqUpKgQtuc72gUOlp6kur6JMQNO7b8vIswYmcS7m/IJCQogMiSQqyZphwl30kSguk1xZS33v7Odz3NKMAaiQwP5eNdRFlyQxn/PGklYcPuPzFfXN/LUihz+vuYAY5NjePamSaf0yEmKDuXJGyZwywsbyCmu4jdzx7T0WV90cwYLXtzAtvyyDrvkdeQCq0pk3YFSBsY7njJdsaeYkf2iSOkT3tFHz0hUaBDXTk7hta8OMTQxgsLyWv7v6rFt7ts/Joz+MWHd+v1xEcH88KIh3XrMzogI41Ji2Wo1GLc0FKec3rV1xsgkXll/iH9vL+SWqYOIbKOBW3Ufn/nbdS7S+zLHoK3e6YlPslm//xg/vmQYcycmMyAmjN9/nMVLa3NZk13C0zdOPK0/u91u+GBLAX9YlkVRRR03ThnIr789uqUx0dk30hN59DvnUFbdwPfOH9SyPizYxqs/OI+KmgbiI8+semNYUiQJkSGs23+MG84dSFl1PZsOneDOdqqEusOCC9J4eV0uj3+Szej+0VYDrG+bkBrDij1FVNQ2sLOgnJDAAIa1Ud01dWg8oUEB1DbYudnpWiv38IlEEBoayrFjx4iPj/fpZNA8H0FoaKinQzlNbulJ3tmUz83nD+JnTo2kD185hktH9+W+t7dx9bNr+eUVo7j5/EEYA8v3FPHnz3LYWVDB+NRYnr1pUkvVSHtuOq/tm0KQLeCMkwA4fq2ePySOtVY7waq9JTTZDTNHua9ONy0hgpkjk1ixp5g7pw/16X+7zcalxAKwI7+cnUfKGdU/+pQutM1Cg2x8e9wAKmobSO/rXT1sfJFPJIKUlBTy8/P9Ypz+5hnKvM1Tn+YQZBN+dMnpv6CnDUtg6b3f4L63t/LQh7tYmVVMQVkN2UVVDIwL58kbxjN3fPIZ9wHvLhcMTeCj7YUcKD3J8j1FJESGMN66cbnLA7NGMiQxkss76bLpK8ZZ1UBb88rYVVDB3IkD2t33sevG91RYfs8nEkFQUJDO2OUG+4oryS2tZuaopA5/reYUVfLPrQXcdtEQkqLaLq3ERQTzj/nn8tznB3hs2V6GJEbw1LwJzBnbv81fhJ7Q3HVyTXYJa/aWMHtsP7cnp+F9o/jF5WffD763iA0PJi3e8SxHZV0j5wxwbegL5V4+kQiUa55Ynk1u6UnHmClRwUxJiyMj7fSqGGMML6/N5bf/yaK+0c7EgbH86orR7Y5x8uSKbCKCA7njoo7r0wMChNsvHsoN56YSHRrk8RJAa2nx4fSPCWXRmgNU1jUyc1RfT4fkk8anxvLh1iMALo+BpNxLE4Gf+HjnUZ7+NIekqBCq6hqprm8CYEpaHHfNGMa0ofEcr66nuKKOP36yl5V7S5gxMomZo5J4akUOVz+7lilpcTQZQ2VtA3WNdiKCA4kMDWTDwePcM2MYfSJcm+DDXROBnC0RYeqQeN7fUkBwYEC3dNdUpxuf4kgEwbYAhmv9v1fQROAHKmob+PWSnYzqH82Su6cRZAugoraB9zbls2jNAea/sOGU/YMDA3hk7hhuPn8QIsJ3JiTzt9X7WZNTSmSIjcTISIIDAzhZ10hlbSPfSE/g1m/0bFdEd5k61JEIpg2NJzxY/3u4w/hURylgRL+oHpl0RXVO/6X7gcc+3ktxZR2Lbv56wK7o0CC+P20wN503iA+3FpB3oobEyGASIkM4JznmlD78ESGB3HfZiA4nC/EVF6YnEGwLYM649hsx1dkZMyCGIJtotZAX0UTg4zYdOsGrXx1i/tS0U4b/bRYcGMB1HYxk6W/6x4SxduEM4l2s5lJdFxpk44UF5zIs6cyHy1DdSxOBD2tssvP/PthBv+hQt45l7msSzuJ5BOWab6T7/sNzvYlW0Pmw9zbnk3W0kl9dMVof0VdKtUsTgY+qqW/iyeU5TEiNZXYb4/UrpVQzTQQ+6qW1uRytqGXh7JF+MXSBUurMuTURiMgsEdkrIvtE5ME2tg8SkU9FZLuIrBIR7xs7oRcqq67n2VX7mDkyifOGnN1EI0op3+e2RCAiNuAZYDYwGrhRREa32u1xYLExZhzwCPB/7orHnzy7aj9VdY08MGukp0NRSvUC7iwRTAH2GWMOGGPqgTeBua32GQ18Zr1f2cZ21UXZRZW8tDaXqyemMKKfPrWplOqcOxNBMpDntJxvrXO2Dbjaen8VECUip9VliMhtIpIpIpn+MMLomaptaOKeN7YQHRrIg7O1NKCUco2nG4vvBy4WkS3AxUAB0NR6J2PMImNMhjEmIzFR+x+357Fle8k6Wslj1453+/yzSinf4c7O5QWA8yOrKda6FsaYI1glAhGJBK4xxpS5MSaftSa7hH98cZD5UwdxyUjvnCBbKeWd3Fki2Aiki8hgEQkG5gFLnHcQkQQRaY5hIfCCG+PxWftLqrjvnW0M7xvJQj8a214p1T3clgiMMY3A3cAyYA/wtjFml4g8IiJXWrtNB/aKSDbQF/hfd8Xjq746cIyrn12L3W74842T2pzrVymlOiLePBl6WzIyMkxmZqanw/AKH24t4OfvbCclLoyXFkxhYHx45x9SSvklEdlkjMloa5sOQNNLrc4u4d43tzJlcByLbp7stZO9KKW8nyaCXqiytoGF721nWFIki/9rilYHKaXOiiaCXuj3H2dRWFHLe3deoElAKXXWPP0cgeqidfuP8er6w9w6bXC7k8krpVRXaInAyzXZDWv3l3L8ZD2VtY0sWnOAQfHhfjFtpFKqZ2gi8HJvbczjFx/saFkOD7bx4oJzCQvWKiGlVPfQRODl3tp4mOF9I3n2pklEhwYRHRak7QJKqW6lbQReLLuokm355VyfkcqwpCiSokM1CSilup0mAi/2TmYegQHCVRNbD9qqlFLdRxOBl2posvPBlgJmjkoiPlJHElVKuY8mAi+1MquY0qp6rpuc2vnOSil1FjQReKl3NuWTGBXC9BE6/4JSyr00EXihkso6Pssq5uqJyQTa9BIppdxL7zJe6Inl2TTZDddlpHg6FKWUH9BE4GVeXX+INzYc5vaLhzAsSSefV0q5nyYCL/LVgWM8vGQXl4xI5IFv6eTzSqmeoYnASxSU1fCj1zYzMD6cp26ciC1APB2SUspPaCLwEs+tOUBVXSPP3ZJBdGiQp8NRSvkRTQReYk12CRcMjWdoYqSnQ1FK+RlNBF7g8LFqDpSe5OLh+syAUqrnaSLwAqtzSgC4eESShyNRSvkjTQReYPXeEgbGhZMWH+7pUJRSfkgTgYfVN9pZu7+Ui4cnIqI9hZRSPU8TgYdlHjpOdX2Ttg8opTxGE4GHrc4uIcgmTB0a7+lQlFJ+ShOBh63eW8K5aXFEhOisoUopz3BrIhCRWSKyV0T2iciDbWwfKCIrRWSLiGwXkcvdGY+3KaqoJetopVYLKaU8ym2JQERswDPAbGA0cKOIjG612y+Bt40xE4F5wLPuiscbrc5u7jaqiUAp5TnuLBFMAfYZYw4YY+qBN4G5rfYxQLT1PgY44sZ4vM7qvSX0jQ5hRF8dZVQp5TnuTATJQJ7Tcr61ztnDwPdEJB9YCvzYjfF4lcYmO2tySpg+PEm7jSqlPMrTjcU3Ai8ZY1KAy4FXROS0mETkNhHJFJHMkpKSHg/SHTYfLqOytlGnolRKeZw7E0EB4Dzzeoq1ztmtwNsAxph1QCiQ0PpAxphFxpgMY0xGYqJv3DhX7S0mMECYln7a6SqlVI9yZyLYCKSLyGARCcbRGLyk1T6HgZkAIjIKRyLwjZ/8nVi1t4RJg/rokNNKKY9zWyIwxjQCdwPLgD04egftEpFHRORKa7f7gB+KyDbgDWCBMca4KyZvUVRRy+7CCq0WUkp5Bbc+xWSMWYqjEdh53UNO73cD09wZgzdavddR6Jk+XEcbVUp5nqcbi/3Squxi+kaHMKq/dhtVSnmeJoIe1thk5/OcUu02qpTyGpoIeph2G1VKeRtNBD1Mu40qpbyNJoIeZIzh411HOTctTruNKqW8hiaCHrS3qJIDJSe5fFx/T4eilFItNBH0oKU7jhIgMGtMP0+HopRSLTQR9KClOwqZMjiOxKgQT4eilFItNBH0kOyiSvYVVzFnrFYLKaW8iyaCHvLv7YWIwLfO0WohpZR30UTQQ5buKOTctDiSokI9HYpSSp2i00QgIt9ua44A5bqcokpytFpIKeWlXLnB3wDkiMgfRGSkuwPyRUt3HEUEZmu1kFLKC3WaCIwx3wMmAvuBl0RknTVjmI6Y5qIv95UyPiWWpGitFlJKeR+XqnyMMRXAuzgmoO8PXAVsFhG/mWP4TBlj2FtUyegB0Z4ORSml2uRKG8GVIvIBsAoIAqYYY2YD43FMLKM6UFJVR3lNA+lJkZ4ORSml2uTKxDTXAE8aY9Y4rzTGVIvIre4Jy3fsK6oCYHhfrUlTSnknVxLBw0Bh84KIhAF9jTG5xphP3RWYr8guqgTQEoFSymu50kbwDmB3Wm6y1ikXZBdXERMWpMNKKKW8liuJINAYU9+8YL0Pdl9IvmVfURXD+0bqbGRKKa/lSiIoEZErmxdEZC5Q6r6QfIcxhuziSoYlafuAUsp7udJGcAfwmoj8BRAgD7jFrVH5iJKqOsqqGxjeV9sHlFLeq9NEYIzZD5wvIpHWcpXbo/IRzT2G0rVEoJTyYq6UCBCROcAYILS5rtsY84gb4/IJzT2GtESglPJmrjxQ9jcc4w39GEfV0HXAIDfH5RNyiquIDg3UHkNKKa/mSmPxBcaYW4ATxpj/AaYCw90blm/IKapieN8o7TGklPJqriSCWuvPahEZADTgGG9IdaC5x1C6VgsppbycK4ngXyISCzwGbAZygdddObiIzBKRvSKyT0QebGP7kyKy1Xpli0iZ66F7t9KqesqqG7ShWCnl9TpsLLYmpPnUGFMGvCciHwGhxpjyzg4sIjbgGeBSIB/YKCJLjDG7m/cxxvzUaf8f4xju2ifktDQUayJQSnm3DksExhg7jpt583KdK0nAMgXYZ4w5YD2N/CYwt4P9bwTecPHYXq9ljCGtGlJKeTlXqoY+FZFrpOstnsk4Hj5rlm+tO42IDAIGA5+1s/02EckUkcySkpIuhuEZzT2GkrTHkFLKy7mSCG7HMchcnYhUiEiliFR0cxzzgHeNMU1tbTTGLDLGZBhjMhITE7v5q90jp7iKdO0xpJTqBVyZqjLKGBNgjAk2xkRby65Mt1UApDotp1jr2jIPH6oWAig4UcPAuHBPh6GUUp3q9MliEbmorfWtJ6ppw0YgXUQG40gA84DvtnH8kUAfYF2n0fYSTXbD0YpaBsTqHMVKKe/nyhATP3d6H4qjEXgTMKOjDxljGkXkbmAZYANeMMbsEpFHgExjzBJr13nAm8YY0+XovVRxZS1NdkP/mDBPh6KUUp1yZdC5bzsvi0gq8CdXDm6MWQosbbXuoVbLD7tyrN7kSJnjGTwtESilegNXGotbywdGdXcgvqSwvAZASwRKqV7BlTaCPwPN1TYBwAQcTxirdhS2lAg0ESilvJ8rbQSZTu8bgTeMMV+6KR6fUFBWQ0SwjehQl0b5Vkopj3LlTvUuUNvcx19EbCISboypdm9ovVdheQ39Y8P0GQKlVK/g0pPFgHMdRxiwwj3h+IbC8lr6x2hDsVKqd3AlEYQ6T09pvdcnpTpwpKyWZG0fUEr1Eq4kgpMiMql5QUQmAzXuC6l3q2tsorSqTnsMKaV6DVfaCH4CvCMiR3BMVdkPx9SVqg1Hyx09hvrrMwRKqV7ClQfKNlrDQIywVu01xjS4N6zeq+VhMi0RKKV6CVcmr78LiDDG7DTG7AQiReRH7g+td2p+mEyfKlZK9RautBH80JqhDABjzAngh26LqJcrbK4a0hKBUqqXcCUR2JwnpbGmoAx2X0i925GyGvqEBxEWbPN0KEop5RJXGos/Bt4Skb9by7cD/3FfSL3bkbIaLQ0opXoVVxLBfwO3AXdYy9tx9BxSbSgsryWljz5moZTqPVyZocwOfAXk4piLYAawx71h9V5Hymq0oVgp1au0WyIQkeHAjdarFHgLwBhzSc+E1vtU1TVSUduoVUNKqV6lo6qhLOBz4ApjzD4AEflpj0TVSxWWaddRpVTv01HV0NVAIbBSRJ4TkZk4nixW7ThSrvMQKKV6n3YTgTHmn8aYecBIYCWOoSaSROSvInJZD8XXqzSXCHTkUaVUb+JKY/FJY8zr1tzFKcAWHD2JVCtHymsRgb7RmgiUUr1Hl+YsNsacMMYsMsbMdFdAvVlhWQ1JUSEE2c5kKmillPIMvWN1oyPlNdo+oJTqdTQRdBNjDLml1TrqqFKq19FE0E2+3HeMgrIaZoxM8nQoSinVJZoIusnidbnERQQzZ1x/T4eilFJdoomgGxSU1bBiTxHXZ6QSGqSjjiqlehe3JgIRmSUie0Vkn4g82M4+14vIbhHZJSKvuzMed3n9q0MY4KbzBno6FKWU6jJXRh89I9a8Bc8AlwL5wEYRWWKM2e20TzqwEJhmjDkhIr2ugr2usYk3N+Qxc2RfUuN01FGlVO/jzhLBFGCfMeaAMaYeeBOY22qfHwLPWLOeYYwpdmM8bvGfHUc5drKeW6YO8nQoSil1RtyZCJKBPKflfGuds+HAcBH5UkTWi8istg4kIreJSKaIZJaUlLgp3DPzyvpDDE6I4MJhCZ4ORSmlzoinG4sDgXRgOo7hrp8TkdjWO1lPM2cYYzISExN7NsIOFFXUsunQCa6dnEJAgI7Hp5TqndyZCAqAVKflFGuds3xgiTGmwRhzEMjGkRh6hdXZjtKJPjuglOrN3JkINgLpIjJYRIKBecCSVvv8E0dpABFJwFFVdMCNMXWr1XtL6Bsdwsh+UZ4ORSmlzpjbEoExphG4G1iGY2rLt40xu0TkERG50tptGXBMRHbjGOr658aYY+6KqTs1Ntn5PKeEi4cnIqLVQkqp3stt3UcBjDFLgaWt1j3k9N4AP7NevcrWvDIqahu5eLhWCymlejdPNxb3WquzS7AFCBema28hpVTvpongDK3aW8LE1FhiwoI8HYpSSp0VTQRnoKSyjh0F5Uwf4T1dWZVS6kxpIjgDn+c4uo1OH6HtA0qp3k8TwRlYtbeEhMhgRveP9nQoSil11jQRdFFzt9GLhifq08RKKZ+giaCLPs8p5UR1A98a08/ToSilVLfQRNBF727Op094EJdo+4BSykdoIuiC8uoGlu8qYu6EZIID9a9OKeUb9G7WBf/afoT6JjvXTk7xdChKKdVtNBF0wXub8xnRN4oxA7S3kFLKd2gicNH+kiq2HC7jmsnJOsicUsqnaCJw0fub8wkQ+M6E1pOsKaVU76aJwAV2u+H9zQVcPDyRpOhQT4ejlFLdShOBC3KKqygsr2XOuAGeDkUppbqdJgIXbM8vA2BCaqxH41BKKXfQROCCHQXlRATbGJIQ4elQlFKq22kicMGOgnLOSY7RsYWUUj5JE0EnGprs7D5SwbiUGE+HopRSbqGJoBM5RVXUNdoZmxLr6VCUUsotNBF0YkdBGQBjk7VEoJTyTZoIOrE9v5yo0EAGxYV7OhSllHILTQSd2FFQzlhtKFZK+TBNBB2ob7STVVjJWG0oVkr5ME0EHcguqqS+yc645FhPh6KUUm6jiaAD2/PLAbTrqFLKp7k1EYjILBHZKyL7ROTBNrYvEJESEdlqvX7gzni6akdBGbHhQaT0CfN0KEop5TaB7jqwiNiAZ4BLgXxgo4gsMcbsbrXrW8aYu90Vx9nYnu9oKNb5B5RSvsydJYIpwD5jzAFjTD3wJjDXjd/XrWobmth7tFKfH1BK+Tx3JoJkIM9pOd9a19o1IrJdRN4VkVQ3xtMluwsraLQbbR9QSvk8TzcW/wtIM8aMA5YDL7e1k4jcJiKZIpJZUlLSI4FtPnQCgEmD+vTI9ymllKe4MxEUAM6/8FOsdS2MMceMMXXW4vPA5LYOZIxZZIzJMMZkJCYmuiXY1jJzTzAwLpykKJ2RTCnl29yZCDYC6SIyWESCgXnAEucdRKS/0+KVwB43xuMyYwybDp9gspYGlFJ+wG2JwBjTCNwNLMNxg3/bGLNLRB4RkSut3e4RkV0isg24B1jgrnjaU9fYxIdbC7DbTcu6/BM1lFTWabWQUsovuK37KIAxZimwtNW6h5zeLwQWujOGzny0rZD73tlGSKCNWef0A2CT1T6QoYlAKeUHPN1Y7HHbrPmI/7nl6+aLzEPHiQwJZHjfKA9FpZRSPUcTQV4ZAJ9lFVNe3QDApkNlTBwYi01HHFVK+QG/TgT1jXb2FFYydUg89U12lu4spLK2gb1HK7ShWCnlN9zaRuDtso5WUN9k56bzB1JcWcsHWwpI7ROO3aCJQCnlN/y6RLDNGl10fEosV01MZsPB43y4tYAAgQmpsZ4NTimleohfJ4LteWXERQST0ieMuRMco1+8uzmfEf2iiQoN8nB0SinVM/w6EWzLL2NcimN00dS4cM5N64MxMHlQrKdDU0qpHuO3ieBkXSP7iqsYnxLbsu47Ex2lAm0fUEr5E79tLN5ZUI7dwPjUr0cXvXpiCmXVDXxrTD8PRqaUUj3LbxPB19NQxrasCwu2cdclwzwUkVJKeYbfVg1tyy8jOTaMhMgQT4eilFIe5deJwLlaSCml/JVfJoLjJ+vJO15zSrWQUkr5K79MBNutgeZ0GkqllPLTRPBFTinBtoBTuo4qpZS/8rtEYIzhk91FTBsWT0SI33aaUkqpFn6XCPYWVXL4eDWX6bMCSikF+GEi+GRXESIwc1SSp0NRSimv4H+JYPdRJg3sQ1JUqKdDUUopr+BXiaCgrIadBRVcNrqvp0NRSimv4VeJYPmuowDaPqCUUk78KhF8sruI9KRIBidEeDoUpZTyGn6TCMqq6/nq4HEuG6PVQkop5cxvEsFnWcU02Q2XjdZqIaWUcuY3iSAqNIhLR/dlbLIOK6GUUs785tHaS0f35VLtLaSUUqfxmxKBUkqptrk1EYjILBHZKyL7ROTBDva7RkSMiGS4Mx6llFKnc1siEBEb8AwwGxgN3Cgio9vYLwq4F/jKXbEopZRqnztLBFOAfcaYA8aYeuBNYG4b+/0G+D1Q68ZYlFJKtcOdiSAZyHNazrfWtRCRSUCqMebfHR1IRG4TkUwRySwpKen+SJVSyo95rLFYRAKAJ4D7OtvXGLPIGJNhjMlITEx0f3BKKeVH3JkICoBUp+UUa12zKOAcYJWI5ALnA0u0wVgppXqWOxPBRiBdRAaLSDAwD1jSvNEYU26MSTDGpBlj0oD1wJXGmEw3xqSUUqoVtz1QZoxpFJG7gWWADXjBGLNLRB4BMo0xSzo+Qts2bdpUKiKHzjCsBKD0DD/bm/njefvjOYN/nrc/njN0/bwHtbdBjDFnH04vISKZxhi/q3ryx/P2x3MG/zxvfzxn6N7z1ieLlVLKz2kiUEopP+dviWCRpwPwEH88b388Z/DP8/bHc4ZuPG+/aiNQSil1On8rESillGpFE4FSSvk5v0kErg6J3ZuJSKqIrBSR3SKyS0TutdbHichyEcmx/uzj6Vi7m4jYRGSLiHxkLQ8Wka+s6/2W9VCjTxGRWBF5V0SyRGSPiEz1k2v9U+vf904ReUNEQn3teovICyJSLCI7nda1eW3F4Wnr3LdbY7h1iV8kAleHxPYBjcB9xpjROIbsuMs6zweBT40x6cCn1rKvuRfY47T8e+BJY8ww4ARwq0eicq+ngI+NMSOB8TjO36evtYgkA/cAGcaYc3A8rDoP37veLwGzWq1r79rOBtKt123AX7v6ZX6RCHB9SOxezRhTaIzZbL2vxHFjSMZxri9bu70MfMcjAbqJiKQAc4DnrWUBZgDvWrv44jnHABcB/wAwxtQbY8rw8WttCQTCRCQQCAcK8bHrbYxZAxxvtbq9azsXWGwc1gOxItK/K9/nL4mg0yGxfY2IpAETcUz409cYU2htOgr42uTNfwIeAOzWcjxQZoxptJZ98XoPBkqAF60qsedFJAIfv9bGmALgceAwjgRQDmzC9683tH9tz/r+5i+JwK+ISCTwHvATY0yF8zbj6C/sM32GReQKoNgYs8nTsfSwQGAS8FdjzETgJK2qgXztWgNY9eJzcSTCAUAEp1eh+Lzuvrb+kgg6GxLbZ4hIEI4k8Jox5n1rdVFzUdH6s9hT8bnBNOBKayjzN3FUETyFo3jcPKiiL17vfCDfGNM8xeu7OBKDL19rgG8CB40xJcaYBuB9HP8GfP16Q/vX9qzvb/6SCDocEttXWHXj/wD2GGOecNq0BJhvvZ8PfNjTsbmLMWahMSbFGsp8HvCZMeYmYCVwrbWbT50zgDHmKJAnIiOsVTOB3fjwtbYcBs4XkXDr33vzefv09ba0d22XALdYvYfOB8qdqpBcY4zxixdwOZAN7Af+n6fjcdM5XoijuLgd2Gq9LsdRZ/4pkAOsAOI8Haubzn868JH1fgiwAdgHvAOEeDo+N5zvBCDTut7/BPr4w7UG/gfIAnYCrwAhvna9gTdwtIE04Cj93dretQUER6/I/cAOHD2quvR9OsSEUkr5OX+pGlJKKdUOTQRKKeXnNBEopZSf00SglFJ+ThOBUkr5OU0ESrUiIk0istXp1W0Dt4lImvOIkkp5g8DOd1HK79QYYyZ4OgileoqWCJRykYjkisgfRGSHiGwQkWHW+jQR+cwaC/5TERlore8rIh+IyDbrdYF1KJuIPGeNqf+JiIR57KSUQhOBUm0Ja1U1dIPTtnJjzFjgLzhGPQX4M/CyMWYc8BrwtLX+aWC1MWY8jnGAdlnr04FnjDFjgDLgGreejVKd0CeLlWpFRKqMMZFtrM8FZhhjDliD+x01xsSLSCnQ3xjTYK0vNMYkiEgJkGKMqXM6Rhqw3DgmF0FE/hsIMsY82gOnplSbtESgVNeYdt53RZ3T+ya0rU55mCYCpbrmBqc/11nv1+IY+RTgJuBz6/2nwJ3QMqdyTE8FqVRX6C8RpU4XJiJbnZY/NsY0dyHtIyLbcfyqv9Fa92McM4X9HMesYd+31t8LLBKRW3H88r8Tx4iSSnkVbSNQykVWG0GGMabU07Eo1Z20akgppfyclgiUUsrPaYlAKaX8nCYCpZTyc5oIlFLKz2kiUEopP6eJQCml/Nz/B/qUFXdso/cFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_predictions = model.predict(test_images)\n",
    "test_predictions = tf.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Compute the evaluation metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision = precision_score(test_labels, test_predictions, average='macro')\n",
    "test_recall = recall_score(test_labels, test_predictions, average='macro')\n",
    "test_f1_score = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Test accuracy: {test_accuracy:.2f}')\n",
    "print(f'Test precision: {test_precision:.2f}')\n",
    "print(f'Test recall: {test_recall:.2f}')\n",
    "print(f'Test F1-score: {test_f1_score:.2f}')\n",
    "\n",
    "# Plot the model's learning metrics\n",
    "#history = model.history\n",
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
